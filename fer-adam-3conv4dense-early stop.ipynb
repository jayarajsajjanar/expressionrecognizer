{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook trains the model with 3 convolutional layers, 4 densely connected layers including the final output softmax layer. Each of the 3 convolutional layers are followed by by a batch normalization layer and a maxpooling layer.\n",
    "We use adam optimization during training.\n",
    "This particular implementation uses early stopping along with dropout for regularization optimization for training\n",
    "\n",
    "All the layers except the output layer use ReLU activation function to preserve the non-linearity in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend\n",
    "from keras.models import Sequential, \n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from numpy import unique, ndarray, mean, std\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import read_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "vector_encoding = True\n",
    "img_height, img_width = 48, 48\n",
    "filepath = \"adam-early-stopping.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_pickle('input_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.Training\n",
    "validate = data.Validation\n",
    "test = data.Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train\n",
    "x_validate, y_validate = validate\n",
    "x_test, y_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x - mean(x, axis = 1).reshape((-1,1))\n",
    "    x = (x - mean(x, axis = 0)) / std(x, axis = 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = normalize(x_train)\n",
    "x_validate = normalize(x_validate)\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = x_train.shape[0]\n",
    "num_validate_samples = x_validate.shape[0]\n",
    "num_test_samples = x_test.shape[0]\n",
    "num_labels = unique(y_train).shape[0]\n",
    "if(vector_encoding):\n",
    "    y_train = keras.utils.to_categorical(y_train, num_labels)\n",
    "    y_validate = keras.utils.to_categorical(y_validate, num_labels)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(num_train_samples, 1, img_height, img_width)\n",
    "    x_validate = x_validate.reshape(num_validate_samples, 1, img_height, img_width)\n",
    "    x_test = x_test.reshape(num_test_samples, 1, img_height, img_width)\n",
    "    input_shape = (1, img_height, img_width)\n",
    "else:\n",
    "    x_train = x_train.reshape(num_train_samples, img_height, img_width, 1)\n",
    "    x_validate = x_validate.reshape(num_validate_samples, img_height, img_width, 1)\n",
    "    x_test = x_test.reshape(num_test_samples, img_height, img_width, 1)\n",
    "    input_shape = (img_height, img_width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=0, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             period = 1)\n",
    "early_stopping = EarlyStopping(monitor='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_validate, y_validate),\n",
    "                    shuffle=True,\n",
    "                    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, accuracy) = model.evaluate(x_test,\n",
    "                                  y_test,\n",
    "                                  batch_size=batch_size,\n",
    "                                  verbose=1)\n",
    "print(\"Loss: %.4f\"%(loss))\n",
    "print(\"Accuracy: %.4f\"%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0,1)\n",
    "plt.legend(['Train','Validation'], loc='lower right')\n",
    "plt.savefig('Accuracy_adam_early_stopping_3conv_4dense.png')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0,2.5)\n",
    "plt.legend(['Train','Validation'], loc='upper right')\n",
    "plt.savefig('Loss_adam_early_stopping_3conv_4dense.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='adam_early_stopping_3conv_4dense.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
